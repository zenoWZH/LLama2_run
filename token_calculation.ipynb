{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "model_name = \"NousResearch/Llama-2-7b-chat-hf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_dataset_info_traintest(dataset, tokenized_dataset):\n",
    "    total_tokens = 0\n",
    "    for attr in dataset.keys():\n",
    "        print(f\"Dataset {attr} has {len(dataset[attr])} examples.\")\n",
    "        curr_tokens = sum([len(tokens[\"input_ids\"]) for tokens in tokenized_dataset[attr]])\n",
    "        print(f\"Dataset {attr} has {curr_tokens} tokens.\")\n",
    "        total_tokens += curr_tokens\n",
    "\n",
    "    print(f\"Total number of tokens in the dataset: {total_tokens}\")\n",
    "    return total_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_dataset_info(tokenized_dataset):\n",
    "    total_tokens = 0\n",
    "    attr = \"text\"\n",
    "    #for attr in tokenized_dataset.features.keys():\n",
    "    print(f\"Dataset {attr} has {len(tokenized_dataset[attr])} examples.\")\n",
    "    curr_tokens = sum([len(tokens[\"input_ids\"]) for tokens in tokenized_dataset])\n",
    "    print(f\"Dataset {attr} has {curr_tokens} tokens.\")\n",
    "    total_tokens += curr_tokens\n",
    "\n",
    "    print(f\"Total number of tokens in the dataset: {total_tokens}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test imdb dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '[CLS] I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.', 'label': 0, 'input_ids': [1, 518, 6154, 29903, 29962, 306, 364, 14927, 306, 13862, 315, 15551, 29949, 3308, 29899, 29979, 29923, 2208, 9806, 515, 590, 4863, 3787, 1363, 310, 599, 278, 19341, 29891, 393, 22047, 372, 746, 372, 471, 937, 5492, 297, 29871, 29896, 29929, 29953, 29955, 29889, 306, 884, 6091, 393, 472, 937, 372, 471, 25291, 491, 501, 29889, 29903, 29889, 2888, 29879, 565, 372, 3926, 1898, 304, 3896, 445, 4234, 29892, 5480, 1641, 263, 13524, 310, 12298, 5545, 376, 1285, 307, 874, 616, 29908, 306, 2289, 750, 304, 1074, 445, 363, 6142, 19423, 1182, 2900, 29966, 1182, 2900, 1576, 6492, 338, 24764, 2820, 263, 4123, 21892, 18621, 8368, 4257, 365, 2386, 1058, 10753, 304, 5110, 4129, 1183, 508, 1048, 2834, 29889, 512, 3153, 1183, 10753, 304, 8569, 902, 1098, 296, 1080, 304, 3907, 777, 2656, 310, 1842, 653, 373, 825, 278, 6588, 3925, 2742, 2714, 1048, 3058, 8604, 5626, 1316, 408, 278, 18444, 3362, 322, 8175, 5626, 297, 278, 3303, 3900, 29889, 512, 1546, 6721, 2832, 14722, 322, 15311, 972, 466, 575, 310, 17920, 1048, 1009, 26971, 373, 22661, 29892, 1183, 756, 7916, 411, 902, 18621, 15703, 29892, 770, 29885, 1078, 29892, 322, 8300, 1757, 19423, 1182, 2900, 29966, 1182, 2900, 5618, 413, 6090, 592, 1048, 306, 13862, 315, 15551, 29949, 3308, 29899, 29979, 29923, 2208, 9806, 338, 393, 29871, 29946, 29900, 2440, 8020, 29892, 445, 471, 5545, 1277, 29876, 12122, 29889, 830, 635, 29892, 278, 7916, 322, 302, 566, 537, 20407, 526, 2846, 322, 2215, 1546, 29892, 1584, 769, 372, 29915, 29879, 451, 10322, 763, 777, 28773, 368, 1754, 1277, 1217, 29889, 5806, 590, 2613, 509, 28438, 3458, 1284, 372, 19253, 292, 29892, 297, 16832, 7916, 322, 302, 566, 537, 526, 263, 4655, 380, 481, 280, 297, 21892, 24615, 29889, 7753, 22607, 3034, 9827, 1171, 29892, 1852, 29884, 2197, 1009, 1234, 304, 1781, 2030, 8023, 2259, 14601, 29892, 750, 7916, 20407, 297, 670, 12298, 19423, 1182, 2900, 29966, 1182, 2900, 29902, 437, 844, 355, 278, 2706, 29885, 21079, 363, 278, 2114, 393, 738, 7916, 4318, 297, 278, 2706, 338, 4318, 363, 1616, 4695, 11976, 3265, 1135, 925, 304, 19253, 2305, 322, 1207, 6909, 304, 367, 4318, 297, 1277, 29876, 12122, 278, 10412, 297, 6813, 29889, 306, 13862, 315, 15551, 29949, 3308, 29899, 29979, 29923, 2208, 9806, 338, 263, 1781, 2706, 363, 5019, 24507, 304, 6559, 278, 27654, 322, 3104, 20452, 313, 1217, 6035, 9146, 29897, 310, 21892, 24615, 29889, 1205, 2289, 29892, 445, 2706, 1838, 29915, 29873, 505, 1568, 310, 263, 6492, 29889], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "model_name = \"NousResearch/Llama-2-7b-chat-hf\"\n",
    "dataset_name = \"imdb\"\n",
    "\n",
    "# 定义自定义的格式化函数\n",
    "def custom_formatting(example):\n",
    "    # 这里以添加前缀为例\n",
    "    return {\"text\": \"[CLS] \" + example[\"text\"]}\n",
    "\n",
    "# 加载数据集，这里以\"huggingface/datasets\"中的\"imdb\"数据集为例\n",
    "dataset = load_dataset(dataset_name)\n",
    "\n",
    "# 加载预训练的tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "# 应用自定义的格式化函数\n",
    "formatted_dataset = dataset.map(custom_formatting)\n",
    "\n",
    "# 然后使用tokenizer对处理过的文本进行tokenize处理\n",
    "tokenized_dataset = formatted_dataset.map(lambda examples: tokenizer(examples[\"text\"]), batched=True)\n",
    "\n",
    "# 查看处理后的结果\n",
    "print(tokenized_dataset[\"train\"][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset train has 25000 examples.\n",
      "Dataset train has 8561184 tokens.\n",
      "Dataset test has 25000 examples.\n",
      "Dataset test has 8363244 tokens.\n",
      "Dataset unsupervised has 50000 examples.\n",
      "Dataset unsupervised has 17182234 tokens.\n",
      "Total number of tokens in the dataset: 34106662\n"
     ]
    }
   ],
   "source": [
    "display_dataset_info(dataset, tokenized_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculation of current datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### guanaco-1k\n",
    "Link : https://huggingface.co/datasets/timdettmers/openassistant-guanaco\n",
    "Fields: train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset train has 1000 examples.\n",
      "Dataset train has 461908 tokens.\n",
      "Total number of tokens in the dataset: 461908\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"mlabonne/guanaco-llama2-1k\")\n",
    "tokenized_dataset = dataset.map(lambda examples: tokenizer(examples[\"text\"]), batched=True)\n",
    "display_dataset_info(dataset, tokenized_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset train has 9846 examples.\n",
      "Dataset train has 4357287 tokens.\n",
      "Dataset test has 518 examples.\n",
      "Dataset test has 231497 tokens.\n",
      "Total number of tokens in the dataset: 4588784\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"timdettmers/openassistant-guanaco\")\n",
    "tokenized_dataset = dataset.map(lambda examples: tokenizer(examples[\"text\"]), batched=True)\n",
    "display_dataset_info(dataset, tokenized_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MMLU Dataset 100k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"cais/mmlu\"\n",
    "dataset = load_dataset(dataset_name,  name = \"all\",  split=\"auxiliary_train\")\n",
    "\n",
    "def custom_formatting(example):\n",
    "    output_text = f'''### Question: {example['question']}\\n \n",
    "                    ### Choices: {example['choices']}\\n \n",
    "                    ### Answer: {example['answer']}'''\n",
    "    return {\"text\":output_text}\n",
    "\n",
    "# 加载预训练的tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "# 应用自定义的格式化函数\n",
    "formatted_dataset = dataset.map(custom_formatting)\n",
    "\n",
    "# 然后使用tokenizer对处理过的文本进行tokenize处理\n",
    "tokenized_dataset = formatted_dataset.map(lambda examples: tokenizer(examples[\"text\"]), batched=True)\n",
    "\n",
    "#tokenized_dataset['input_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'subject', 'choices', 'answer', 'text'],\n",
       "    num_rows: 99842\n",
       "})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'subject', 'choices', 'answer', 'text', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 99842\n",
       "})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"### Question: Davis decided to kill Adams. He set out for Adams's house. Before he got there he saw Brooks, who resembled Adams. Thinking that Brooks was Adams, Davis shot at Brooks. The shot missed Brooks but wounded Case, who was some distance away. Davis had not seen Case. In a prosecution under a statute that proscribes any attempt to commit murder, the district attorney should indicate that the intended victim(s) was/were\\n \\n                    ### Choices: ['Adams only.', 'Brooks only.', 'Case only.', 'Adams and Brooks']\\n \\n                    ### Answer: 1\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset text has 99842 examples.\n",
      "Dataset text has 41173458 tokens.\n",
      "Total number of tokens in the dataset: 41173458\n"
     ]
    }
   ],
   "source": [
    "display_dataset_info(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### databricks-dolly-15k "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"databricks/databricks-dolly-15k\"\n",
    "dataset = load_dataset(dataset_name, 'all')\n",
    "\n",
    "def custom_formatting(example):\n",
    "    output_text = f'''### Instruction: {example['instruction']}\\n \n",
    "                    ### Context: {example['context']}\\n \n",
    "                    ### Answer: {example['response']}'''\n",
    "    return {\"text\":output_text}\n",
    "\n",
    "# 加载预训练的tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "# 应用自定义的格式化函数\n",
    "formatted_dataset = dataset.map(custom_formatting)\n",
    "\n",
    "# 然后使用tokenizer对处理过的文本进行tokenize处理\n",
    "tokenized_dataset = formatted_dataset.map(lambda examples: tokenizer(examples[\"text\"]), batched=True)\n",
    "\n",
    "#tokenized_dataset['input_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['instruction', 'context', 'response', 'category', 'text'],\n",
       "        num_rows: 15011\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset train has 15011 examples.\n",
      "Dataset train has 3348888 tokens.\n",
      "Total number of tokens in the dataset: 3348888\n"
     ]
    }
   ],
   "source": [
    "display_dataset_info_traintest(dataset, tokenized_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BelleGroup/train_2M_CN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "657982b1b00a4f8d9efe3cb3d2e9f604",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0729e0648e874d77943a0b4113fc2fe8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_name = \"BelleGroup/train_2M_CN\"\n",
    "dataset = load_dataset(dataset_name, 'all')\n",
    "\n",
    "def custom_formatting(example):\n",
    "    output_text = f'''### Instruction: {example['instruction']}\\n \n",
    "                    ### Answer: {example['output']}'''\n",
    "    return {\"text\":output_text}\n",
    "\n",
    "# 加载预训练的tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "# 应用自定义的格式化函数\n",
    "formatted_dataset = dataset.map(custom_formatting)\n",
    "\n",
    "# 然后使用tokenizer对处理过的文本进行tokenize处理\n",
    "tokenized_dataset = formatted_dataset.map(lambda examples: tokenizer(examples[\"text\"]), batched=True)\n",
    "\n",
    "#tokenized_dataset['input_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset train has 2000000 examples.\n"
     ]
    }
   ],
   "source": [
    "display_dataset_info_traintest(dataset, tokenized_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0.5M CN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b47cf79f1b644da848e795523bf35dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/940 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be9bb254676f44428287b6bbaf447522",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "394154ddcb4d470ea58d6f045388575f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/286M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a03e7c49302849bc863900d1ed53d7c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae612358388d4d4d90f4bb9dbc1b6b30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cfceb748e7341bb8b142bc828d26c3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/519255 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a1c7a838c9c413692adc1c16e8928e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/519255 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_name = \"BelleGroup/train_0.5M_CN\"\n",
    "dataset = load_dataset(dataset_name, 'all')\n",
    "\n",
    "def custom_formatting(example):\n",
    "    output_text = f'''### Instruction: {example['instruction']}\\n \n",
    "                    ### Answer: {example['output']}'''\n",
    "    return {\"text\":output_text}\n",
    "\n",
    "# 加载预训练的tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "# 应用自定义的格式化函数\n",
    "formatted_dataset = dataset.map(custom_formatting)\n",
    "\n",
    "# 然后使用tokenizer对处理过的文本进行tokenize处理\n",
    "tokenized_dataset = formatted_dataset.map(lambda examples: tokenizer(examples[\"text\"]), batched=True)\n",
    "\n",
    "#tokenized_dataset['input_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset train has 519255 examples.\n",
      "Dataset train has 132636429 tokens.\n",
      "Total number of tokens in the dataset: 132636429\n"
     ]
    }
   ],
   "source": [
    "display_dataset_info_traintest(dataset, tokenized_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### m-a-p/COIG-CQIA 中文质量数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"m-a-p/COIG-CQIA\"\n",
    "loadlist = ['chinese_traditional', 'coig_pc', 'exam', 'finance', 'douban', 'human_value', 'logi_qa', 'ruozhiba', 'segmentfault', 'wiki', 'wikihow', 'xhs', 'zhihu']\n",
    "\n",
    "def custom_formatting(example):\n",
    "    output_text = f'''### Instruction: {example['instruction']}\\n \n",
    "                    ### Answer: {example['output']}'''\n",
    "    return {\"text\":output_text}\n",
    "\n",
    "#tokenized_dataset['input_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset train has 1111 examples.\n",
      "Dataset train has 336754 tokens.\n",
      "Total number of tokens in the dataset: 336754\n",
      "Dataset train has 3000 examples.\n",
      "Dataset train has 607153 tokens.\n",
      "Total number of tokens in the dataset: 607153\n",
      "Dataset train has 4856 examples.\n",
      "Dataset train has 2344175 tokens.\n",
      "Total number of tokens in the dataset: 2344175\n",
      "Dataset train has 11288 examples.\n",
      "Dataset train has 33027362 tokens.\n",
      "Total number of tokens in the dataset: 33027362\n",
      "Dataset train has 3086 examples.\n",
      "Dataset train has 2591552 tokens.\n",
      "Total number of tokens in the dataset: 2591552\n",
      "Dataset train has 1007 examples.\n",
      "Dataset train has 351384 tokens.\n",
      "Total number of tokens in the dataset: 351384\n",
      "Dataset train has 421 examples.\n",
      "Dataset train has 232790 tokens.\n",
      "Total number of tokens in the dataset: 232790\n",
      "Dataset train has 240 examples.\n",
      "Dataset train has 100800 tokens.\n",
      "Total number of tokens in the dataset: 100800\n",
      "Dataset train has 458 examples.\n",
      "Dataset train has 312495 tokens.\n",
      "Total number of tokens in the dataset: 312495\n",
      "Dataset train has 10603 examples.\n",
      "Dataset train has 14460705 tokens.\n",
      "Total number of tokens in the dataset: 14460705\n",
      "Dataset train has 1485 examples.\n",
      "Dataset train has 5663890 tokens.\n",
      "Total number of tokens in the dataset: 5663890\n",
      "Dataset train has 1508 examples.\n",
      "Dataset train has 1387570 tokens.\n",
      "Total number of tokens in the dataset: 1387570\n",
      "Dataset train has 5631 examples.\n",
      "Dataset train has 6443659 tokens.\n",
      "Total number of tokens in the dataset: 6443659\n",
      "########################################################\n",
      "Total number of tokens in this dataset: 67860289\n"
     ]
    }
   ],
   "source": [
    "final_tokens = 0\n",
    "for option in loadlist:\n",
    "\n",
    "    dataset = load_dataset(dataset_name, option)\n",
    "    formatted_dataset = dataset.map(custom_formatting)\n",
    "    tokenized_dataset = formatted_dataset.map(lambda examples: tokenizer(examples[\"text\"]), batched=True)\n",
    "    final_tokens += display_dataset_info_traintest(dataset, tokenized_dataset)\n",
    "    \n",
    "print(\"########################################################\")\n",
    "print(f\"Total number of tokens in this dataset: {final_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['conversations', 'id'],\n",
       "        num_rows: 3606402\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef2b075b181c4f3b8bb1c08253815348",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3606402 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_name = \"BelleGroup/train_3.5M_CN\"\n",
    "dataset = load_dataset(dataset_name, 'all')\n",
    "\n",
    "def custom_formatting(example):\n",
    "    output_text = \"\"\n",
    "    for round in example['conversations']:\n",
    "        output_text += f'''### From: {round['from']}\\n \n",
    "                    ### Text: {round['value']}'''\n",
    "    return {\"text\":output_text}\n",
    "\n",
    "# 加载预训练的tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "# 应用自定义的格式化函数\n",
    "formatted_dataset = dataset.map(custom_formatting)\n",
    "\n",
    "# 然后使用tokenizer对处理过的文本进行tokenize处理\n",
    "tokenized_dataset = formatted_dataset.map(lambda examples: tokenizer(examples[\"text\"]), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset train has 3606402 examples.\n",
      "Dataset train has 2269625936 tokens.\n",
      "Total number of tokens in the dataset: 2269625936\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2269625936"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display_dataset_info_traintest(dataset, tokenized_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310_Yival_benchmark",
   "language": "python",
   "name": "py310_yival_benchmark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
